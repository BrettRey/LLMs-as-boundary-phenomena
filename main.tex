\documentclass[12pt]{article}

% House style preamble
\input{.house-style/preamble.tex}

% Update PDF metadata
\hypersetup{
  pdftitle={LLMs as boundary phenomena}
}

\title{LLMs as boundary phenomena}
\author{Brett Reynolds \orcidlink{0000-0003-0073-7195}%
\thanks{Contact: \href{mailto:brett.reynolds@humber.ca}{brett.reynolds@humber.ca}}\\
Humber Polytechnic \& University of Toronto}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
The debate over whether large language models \enquote{really think} reproduces a familiar pattern: a boundary case meets a binary classification. \textcite{nefdt2026} organizes this debate through a $2\times 2$ matrix crossing language with cognition, placing LLMs in a \enquote{missing quadrant} of language without cognition. But the binary frame undersells his own insights. His hedging on proxies, partial capacities, and qualified attributions points toward something the table can't express: that language and cognition are cluster concepts with graded membership. \term{Homeostatic property cluster} (HPC) theory and projection-purpose analysis offer a better framework. Cognitive predicates serve purposes that converge for core cases but diverge at boundaries~-- the same phenomenon that makes a tomato a vegetable or a fruit depending on whether you're a greengrocer or a botanist. The debate persists because the purpose divergence is invisible in the core case, and resists resolution because disputants treat cluster kinds as if they had essences.
\end{abstract}

\section{Introduction}

Everyone has an opinion about whether ChatGPT \enquote{really thinks.} The debate reproduces a pattern familiar from philosophy of science: a boundary case meets a binary classification, and the deadlock gets mistaken for a factual disagreement.

\textcite{nefdt2026} offers a sophisticated analysis. He frames the conceptual landscape through a $2\times 2$ matrix (his Table~1) crossing language with cognition. Humans have both. Non-human animals have cognition without language. LLMs, Nefdt argues, fill the \enquote{missing quadrant}: language without cognition (p.~4). They're \enquote{purely linguistic agents unplugged from integration with both larger cognitive structure and the world in which it evolved} (p.~12).

The table carves out genuine conceptual space and steers between the eliminativism of \textcite{bender2020} and the maximalism of \textcite{cappelen2025}. But its binary structure undersells Nefdt's own insights. His discussion is full of hedging, proxies, and qualified answers~-- scalar claims that a binary frame can't capture.

This note argues that \term{homeostatic property cluster} (HPC) theory \citep{boyd1991, boyd1999} and projection-purpose analysis handle the LLM case better. HPC theory deals with kinds defined not by necessary and sufficient conditions but by clusters of co-occurring properties held together by causal mechanisms. Which properties matter depends on the projection purpose~-- what the categorization is for. This explains why the debate is so persistent. Nefdt's hedging is a feature, not a bug.

\section{The binary and its discontents}

Nefdt's Table~1 is a $2\times 2$ matrix crossing language with cognition:

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\toprule
 & +Cognition & --Cognition \\
\midrule
+Language & Humans & ??? \\
--Language & Non-human animals & Rocks \\
\bottomrule
\end{tabular}
\caption{Conceptual possibilities, adapted from \textcite{nefdt2026}, p.~4.}
\label{tab:nefdt}
\end{table}

The open question is whether anything occupies the top-right cell: language without cognition. Nefdt's answer is that LLMs fill it. They \enquote{occupy a hitherto vacant part of conceptual space} (p.~4). The table presupposes that \term{language} and \term{cognition} are necessary-and-sufficient-condition categories. Something either has language or it doesn't, either has cognition or it doesn't, and the philosophical work consists in deciding which cell a system occupies. One might object that the table is scaffolding, not a metaphysical commitment~-- a heuristic to organize the debate, not a claim that the categories are really binary. But even heuristic binaries constrain the conceptual space they organize: a table with two columns invites two-valued answers. The hedging that runs through Nefdt's discussion is his own analysis pressing against that constraint.

Consider the evidence. His \textsc{no brainer} principle states that \enquote{LLMs only model one aspect of cognition, namely (statistical) linguistic processing} (p.~6). This already blurs the distinction between columns: if statistical linguistic processing is an \enquote{aspect of cognition,} then LLMs don't simply lack cognition. They instantiate part of it.

\textsc{Cognition unplugged} goes further, drawing on \posscite{casto2025} distinction between \enquote{linguistic understanding} and \enquote{deep understanding.} Nefdt concludes that purely linguistic agents have \enquote{statistically-based proxies for more cognitively loaded states} (p.~8). But proxies aren't absences. This isn't just epistemic hedging (we're uncertain which cell LLMs belong in). It's evidence of ontological continuity: a proxy for reasoning is on the same continuum as reasoning, better explained by continuity than by binary uncertainty.

The hedging continues throughout section~4. On perspective: LLMs can be \enquote{trained to execute a particular point of view} (p.~10), but \enquote{the individual phenomenal level is missing} (p.~10). On time: they don't clearly have or lack temporal cognition; rather, \enquote{they could depending on the kinds of structures they employ} (p.~12). On cognitive agency generally: Nefdt titles his section~4.3 \enquote{Why I'm neither a realist nor an eliminativist} and locates himself in \enquote{a nonempty position in between} the two poles (p.~12).

These aren't binary verdicts. They're positions along a continuum. The properties Nefdt distributes between \enquote{language} and \enquote{cognition} (inferential reasoning, perspective-taking, temporal processing, phenomenal experience) don't sort cleanly into two groups. They cluster, with graded and contested membership at the margins. As \textcite{mahowald2024} show, language and thought \enquote{dissociate} in LLMs, but the dissociation is partial and uneven~-- not a clean binary divide.

\section{HPC reframing}

HPC theory was developed by \textcite{boyd1991, boyd1999} to handle \term{natural kinds} that resist definition by necessary and sufficient conditions. The framework is contested (see, e.g., \citealt{magnus2014} for objections), but the core insight~-- that kind membership can be graded and mechanism-dependent~-- is widely shared even among critics. Biological species are the paradigm case, and ring species illustrate it vividly. In the \emph{Ensatina} salamander complex of western North America \citep{wake1997}, adjacent populations share morphology, colouration, and reproductive compatibility, but these properties grade continuously around the ring: where the endpoints meet in southern California, the populations can't interbreed despite geographic overlap. There's no point where one species stops and another begins. The cluster holds, but membership is irreducibly graded.

The framework applies equally to language and cognition. In humans, properties like inferential reasoning, pragmatic implicature, analogical mapping, perspective-taking, planning, emotional response, and embodied experience cluster together. They do so because of homeostatic mechanisms: shared neural architecture, developmental trajectories, social interaction patterns, and embodied engagement with the world.

But the clustering is contingent, not definitional. The properties co-occur in humans because of the particular causal structure of human biology and development, and LLMs instantiate a novel combination from this cluster. They display some properties typically associated with cognition (inferential reasoning, analogical mapping, something like perspective-taking) while lacking others (embodied experience, persistent memory, emotional states, autonomous goal-formation). This doesn't make them a clean occupant of a \enquote{missing quadrant.} It makes them a graded member of the cluster: exactly the kind of entity HPC theory was designed to handle and that binary categories systematically misclassify.

\textcite{powell2020} distinguishes \term{convergent} from \term{contingent} properties, which helps clarify what to expect in a novel system. Convergent properties recur across unrelated systems because similar functional pressures produce similar solutions. Pattern extraction and inferential reasoning may be convergent: any system under sufficient pressure to predict and generate natural language is likely to develop something functionally similar. Contingent properties depend on specific implementation history. The absence of embodiment in LLMs is a contingent feature of their engineering, not a deficit. The absence of phenomenal consciousness may be contingent too, or it may reflect deeper architectural constraints~-- an empirical question, not a definitional one. Either way, HPC theory predicts what we observe: a system that shares some cluster properties and lacks others, with the specific combination shaped by causal mechanisms rather than by a definition.

A further dimension this note doesn't develop is etiological: it matters not just which properties cluster but how they came to do so. Boyd's HPC framework was designed for natural kinds whose clustering arises from natural causal processes; in LLMs, the clustering is engineered. Whether this difference undermines the analogy is a question for another paper.

\section{The tomato move}\label{sec:tomato}

HPC theory handles the clustering. But there's a second problem with the binary: even granting that LLMs occupy some definite cell, we'd need to ask, definite relative to what?

To a greengrocer, a tomato is a vegetable: it's savoury, shelved with the peppers and onions. To a botanist, it's a fruit: it develops from the ovary of a flower and contains seeds. Neither classification is wrong. Each serves a different \term{projection purpose}: the interest or analytical goal that determines which similarities count and therefore what falls inside the category. The disagreement dissolves once you specify which purpose you're serving. But perspectival doesn't mean inconsequential. The US Supreme Court ruled in \emph{Nix v.\ Hedden} (1893) that tomatoes are vegetables for tariff purposes~-- a projection-purpose dispute with real economic stakes. Whether LLMs \enquote{really think} has analogous consequences for regulation, liability, and intellectual-property law.

In \posscite{Goodman1955} terms, the issue is about projectibility: not all predicates project equally well to new cases. \mention{Green} projects from observed emeralds to unobserved ones; \mention{grue} doesn't. A predicate is projectible when it lets you predict further properties of new instances. When cognitive scientists apply false-belief tasks to LLMs \citep{kosinski2024}~-- the same paradigm developed for chimpanzees \citep{premack1978} and children \citep{wimmer1983}~-- they're projecting \mention{cognitive}, and it predicts some capacities well. When \textcite{kallini2024} find that LLMs struggle with impossible languages but handle natural ones readily, they're projecting \mention{linguistic}, and it predicts others. For LLMs, the question isn't which predicate applies but which one projects usefully, and for whom. Each analytical perspective yields a different answer.

Under a neuroscience projection, we ask what mechanism produces the behaviour. LLMs process tokenized text via vector operations and statistical pattern-matching. By this criterion, their capacities are linguistic: produced by something analogous to a language-processing system, not by the full suite of cognitive machinery.

Under a functional projection, we ask what the system does. LLMs draw inferences, construct analogies, adopt perspectives, and plan multi-step solutions. By this criterion, their capacities are cognitive: they exhibit the functional profile of cognition regardless of the underlying mechanism.

Under a phenomenological projection, we ask whether there is something it is like to be the system \citep{nagel1974}. The answer for LLMs is probably no, or at least undecidable from the outside. By this criterion, the question of cognition can't be resolved.

Nefdt's Table~1 implicitly places more weight on what produces the behaviour than on what the behaviour achieves. LLMs lack the neural substrates and embodied integration associated with cognition in humans, so they go in the \enquote{language without cognition} cell. But this is a consequence of the projection chosen, not a discovery about LLMs. Under the functional projection, the same systems would land in \enquote{language and cognition.}

The persistence of the \enquote{do LLMs really think?} debate is predicted by projection-purpose analysis. It's a dispute about which predicate to project, disguised as an ontological one. Resolving it doesn't require discovering a hidden fact about LLMs. It requires specifying the purpose of the categorization.

\section{Why this matters}

The LLM case isn't just applied philosophy of AI. It's a publicly accessible illustration of how projection purposes shape categorization. The intuitions are already there, which makes LLMs an unusually vivid test case for the general point.

Nefdt's analysis is better served by the HPC framework than by his table. His hedging, his \enquote{neither realist nor eliminativist} stance, his acknowledgment of proxies and partial capacities~-- these are exactly what HPC theory predicts for boundary cases. The framework doesn't force him to choose a cell. It lets him say what he already wants to say: that LLMs share some cluster properties with cognitive agents, lack others, and that the combination is genuinely novel. The binary table does have one virtue: it forces a commitment. HPC's flexibility is also its risk, since a framework that accommodates everything explains nothing. The claim here is narrower: for \emph{this} boundary case, the cluster structure is more informative than the binary.

Together, the two aspects of the framework diagnose two patterns in the debate. The first explains why the debate persists; the second, why the disagreements within it resist resolution.

The first pattern is projection mismatch. Cognitive predicates like \mention{believes} and \mention{reasons} serve multiple purposes that normally converge. Under one, a predicate projects well when it accurately predicts the cluster of associated properties: calling someone a believer predicts consistency, sincerity, and so on. Under another, it projects well when it provides the right tools for interaction: treating someone as a believer lets you coordinate expectations and hold them accountable. For humans, both purposes agree, so people slide between them without noticing. At the LLM boundary, they come apart. Section~\ref{sec:tomato} showed this at the analytical level, with neuroscience, functional, and phenomenological purposes each placing LLMs differently. The same divergence operates at the level of vocabulary choice. \textcite{shanahan2024} foregrounds predictive accuracy: cognitive predicates carry implications that don't transfer to LLMs, so they risk anthropomorphism and mislead about what to expect. \textcite{cappelen2025} foreground productive engagement: without cognitive predicates, we lack the tools to figure out LLMs' place in our social structures. The observations can largely be shared; what differs is the purpose the predicates serve. Once the purposes are named, the apparent contradiction dissolves into two defensible answers to two different questions.

The second pattern is essentialism about the categories themselves. \textcite{bender2020} and \textcite{piantadosi2022} both ask whether LLMs understand language~-- a shared question under a shared functional projection. But each treats a different property of the meaning cluster as criterial. \textcite{bender2020} take meaning to require a relation between linguistic form and communicative intent, so world-directed grounding becomes the essential property: LLMs lack it, so they don't understand language. \textcite{piantadosi2022} treat meaning as conceptual role constituted by relations among internal representational states, so relational coherence becomes the essential property: LLMs have it, so they do. Each account selects one property from the cluster and elevates it to a necessary condition~-- exactly the move HPC theory diagnoses as an error for cluster kinds. If meaning is a homeostatic property cluster, no single property is definitional; the properties co-occur contingently, held together by mechanisms, and different systems can instantiate different subsets. The disagreement persists because both sides are essentialists about meaning, and an essentialist frame makes a perspectival choice look like a factual mistake.

The question isn't whether LLMs \enquote{really} have cognition. It's which properties cluster, under what mechanisms, for what analytical purpose. That's the question HPC theory and projection-purpose analysis are designed to answer, and it has more interesting answers than yes or no.

\bigskip
\section*{Acknowledgements}

This note was drafted with the assistance of Claude Code (Opus 4.6). I have reviewed and revised all content and take full responsibility for the final text.

\newpage
\printbibliography

\end{document}
